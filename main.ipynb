{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ihtanboga/VES-Deep-Learning/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCcSbNQym-c1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, classification_report\n",
        "from sklearn.metrics import matthews_corrcoef, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "#import shap\n",
        "\n",
        "# Veri yükleme\n",
        "df = pd.read_excel('df.xlsx')\n",
        "\n",
        "# Train ve test verilerini ayırma\n",
        "train_data = df[df['region'] == 1].copy()\n",
        "test_data = df[df['region'] == 2].copy()\n",
        "\n",
        "# Define categorical and numerical features first\n",
        "categorical_features = ['Multifocal_PVC', 'Nonsustained_VT', 'gender', 'HTN', 'DM', 'Fullcompansasion']\n",
        "numeric_features = ['pvc_percent', 'PVCQRS', 'EF', 'Age', 'PVC_Prematurity_index', 'QRS_ratio',\n",
        "                    'mean_HR', 'symptom_duration', 'QTc_sinus', 'PVCCI_dispersion',\n",
        "                    'CI_variability', 'PVC_Peak_QRS_duration', 'PVCCI', 'PVC_Compansatory_interval']\n",
        "\n",
        "# Then perform label encoding\n",
        "le = LabelEncoder()\n",
        "for col in categorical_features:\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "\n",
        "\n",
        "# Label Encoding için kategorik değişkenleri dönüştürme\n",
        "#le = LabelEncoder()\n",
        "#for col in categorical_features:\n",
        " #   train_data[col] = le.fit_transform(train_data[col])\n",
        " #   test_data[col] = le.transform(test_data[col])\n",
        "\n",
        "# Standartlaştırma\n",
        "scaler = StandardScaler()\n",
        "train_data[numeric_features] = scaler.fit_transform(train_data[numeric_features])\n",
        "test_data[numeric_features] = scaler.transform(test_data[numeric_features])\n",
        "\n",
        "# X ve y ayırma\n",
        "X_train = train_data[numeric_features + categorical_features]\n",
        "y_train = train_data['Group']\n",
        "X_test = test_data[numeric_features + categorical_features]\n",
        "y_test = test_data['Group']\n",
        "\n",
        "def evaluate_model(y_true, y_pred, y_pred_proba, model_name):\n",
        "    \"\"\"\n",
        "    Model performans metriklerini hesaplar ve yazdırır\n",
        "    \"\"\"\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    auc = roc_auc_score(y_true, y_pred_proba)\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    sensitivity = tp / (tp + fn)\n",
        "    specificity = tn / (tn + fp)\n",
        "    ppv = tp / (tp + fp)\n",
        "    npv = tn / (tn + fn)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    mcc = matthews_corrcoef(y_true, y_pred)\n",
        "\n",
        "    print(f\"\\n{model_name} Performans Metrikleri:\")\n",
        "    print(f\"Accuracy: {accuracy:.3f}\")\n",
        "    print(f\"AUC: {auc:.3f}\")\n",
        "    print(f\"Sensitivity: {sensitivity:.3f}\")\n",
        "    print(f\"Specificity: {specificity:.3f}\")\n",
        "    print(f\"PPV: {ppv:.3f}\")\n",
        "    print(f\"NPV: {npv:.3f}\")\n",
        "    print(f\"F1 Score: {f1:.3f}\")\n",
        "    print(f\"MCC: {mcc:.3f}\")\n",
        "\n",
        "    return auc\n",
        "\n",
        "def plot_roc_curves(models_dict):\n",
        "    \"\"\"\n",
        "    Tüm modeller için ROC eğrilerini çizer\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    for model_name, (model, y_pred_proba) in models_dict.items():\n",
        "        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
        "        auc = roc_auc_score(y_test, y_pred_proba)\n",
        "        plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc:.3f})')\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC Curves for All Models')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYwzecBVnfHZ"
      },
      "source": [
        "# Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8CTP7Pgw1vT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, roc_auc_score, confusion_matrix,\n",
        "    matthews_corrcoef, f1_score, roc_curve\n",
        ")\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import scipy.stats as stats\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# -----------------------------\n",
        "# 1. Veri Yükleme ve Ön İşleme\n",
        "# -----------------------------\n",
        "\n",
        "# Veri yükleme\n",
        "df = pd.read_excel('df.xlsx')\n",
        "\n",
        "# Kategorik ve numerik değişkenleri ayırma\n",
        "categorical_features = ['Multifocal_PVC', 'Nonsustained_VT', 'gender', 'HTN', 'DM', 'Fullcompansasion']\n",
        "numeric_features = [\n",
        "    'pvc_percent', 'PVCQRS', 'EF', 'Age', 'PVC_Prematurity_index',\n",
        "    'QRS_ratio', 'mean_HR', 'symptom_duration', 'QTc_sinus',\n",
        "    'PVCCI_dispersion', 'CI_variability', 'PVC_Peak_QRS_duration',\n",
        "    'PVCCI', 'PVC_Compansatory_interval'\n",
        "]\n",
        "\n",
        "# Label Encoding için kategorik değişkenleri dönüştürme\n",
        "le = LabelEncoder()\n",
        "for col in categorical_features:\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "\n",
        "# Train ve test verilerini ayırma\n",
        "train_data = df[df['region'] == 1].copy()\n",
        "test_data = df[df['region'] == 2].copy()\n",
        "\n",
        "# Standartlaştırma\n",
        "scaler = StandardScaler()\n",
        "train_data[numeric_features] = scaler.fit_transform(train_data[numeric_features])\n",
        "test_data[numeric_features] = scaler.transform(test_data[numeric_features])\n",
        "\n",
        "# X ve y ayırma\n",
        "X_train = train_data[numeric_features + categorical_features]\n",
        "y_train = train_data['Group']\n",
        "X_test = test_data[numeric_features + categorical_features]\n",
        "y_test = test_data['Group']\n",
        "\n",
        "# -----------------------------\n",
        "# 2. Model Performans Fonksiyonu\n",
        "# -----------------------------\n",
        "\n",
        "def evaluate_model(y_true, y_pred, y_pred_proba, model_name):\n",
        "    \"\"\" Model performans metriklerini hesaplar ve yazdırır \"\"\"\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    auc = roc_auc_score(y_true, y_pred_proba)\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "    ppv = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    npv = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    mcc = matthews_corrcoef(y_true, y_pred)\n",
        "\n",
        "    print(f\"\\n{model_name} Performans Metrikleri:\")\n",
        "    print(f\"Accuracy: {accuracy:.3f}\")\n",
        "    print(f\"AUC: {auc:.3f}\")\n",
        "    print(f\"Sensitivity: {sensitivity:.3f}\")\n",
        "    print(f\"Specificity: {specificity:.3f}\")\n",
        "    print(f\"PPV: {ppv:.3f}\")\n",
        "    print(f\"NPV: {npv:.3f}\")\n",
        "    print(f\"F1 Score: {f1:.3f}\")\n",
        "    print(f\"MCC: {mcc:.3f}\")\n",
        "    return auc\n",
        "\n",
        "# -----------------------------\n",
        "# 3. Hyperparameter Tuning ve Model Eğitimi\n",
        "# -----------------------------\n",
        "\n",
        "# Hiperparametre grid'i\n",
        "param_grid = [\n",
        "    {\n",
        "        'penalty': ['l1'],\n",
        "        'C': stats.loguniform(1e-5, 100),\n",
        "        'solver': ['saga'],\n",
        "        'class_weight': ['balanced', None],\n",
        "        'max_iter': [1000],\n",
        "        'random_state': [42]\n",
        "    },\n",
        "    {\n",
        "        'penalty': ['elasticnet'],\n",
        "        'C': stats.loguniform(1e-5, 100),\n",
        "        'solver': ['saga'],\n",
        "        'l1_ratio': stats.uniform(0, 1),\n",
        "        'class_weight': ['balanced', None],\n",
        "        'max_iter': [1000],\n",
        "        'random_state': [42]\n",
        "    },\n",
        "    {\n",
        "        'penalty': ['l2'],\n",
        "        'C': stats.loguniform(1e-5, 100),\n",
        "        'solver': ['saga'],\n",
        "        'class_weight': ['balanced', None],\n",
        "        'max_iter': [1000],\n",
        "        'random_state': [42]\n",
        "    }\n",
        "]\n",
        "\n",
        "# RandomizedSearchCV ile en iyi parametreleri bulma\n",
        "lr = LogisticRegression()\n",
        "lr_random = RandomizedSearchCV(\n",
        "    estimator=lr,\n",
        "    param_distributions=param_grid,\n",
        "    n_iter=100,\n",
        "    cv=5,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "lr_random.fit(X_train, y_train)\n",
        "\n",
        "# En iyi model ile tahmin\n",
        "best_lr = lr_random.best_estimator_\n",
        "y_pred_lr = best_lr.predict(X_test)\n",
        "y_pred_proba_lr = best_lr.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Model performansını değerlendirme\n",
        "auc_lr = evaluate_model(y_test, y_pred_lr, y_pred_proba_lr, \"Logistic Regression\")\n",
        "\n",
        "# En iyi parametreleri yazdırma\n",
        "print(\"\\nEn İyi Logistic Regression Parametreleri:\")\n",
        "print(lr_random.best_params_)\n",
        "\n",
        "# Sonuçları kaydetme\n",
        "results_lr = pd.DataFrame({\n",
        "    'True_Labels': y_test,\n",
        "    'Predicted_Probabilities': y_pred_proba_lr\n",
        "})\n",
        "results_lr.to_csv('logistic_regression_results.csv', index=False)\n",
        "\n",
        "# -----------------------------\n",
        "# 4. SHAP Analizi\n",
        "# -----------------------------\n",
        "\n",
        "# SHAP Analizi\n",
        "# LinearExplainer Logistic Regression için uygundur\n",
        "explainer = shap.LinearExplainer(best_lr, X_train)\n",
        "\n",
        "# SHAP değerlerini hesapla\n",
        "shap_values = explainer.shap_values(X_test)\n",
        "\n",
        "# SHAP Summary Plot\n",
        "shap.summary_plot(shap_values, X_test, feature_names=X_test.columns)\n",
        "\n",
        "# SHAP Feature Importance (Bar Plot)\n",
        "shap.summary_plot(shap_values, X_test, feature_names=X_test.columns, plot_type=\"bar\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zT8Yn8sWxJ6Q",
        "outputId": "f8453612-2066-467b-e078-1a25f8f65e80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model, scaler ve encoder başarıyla dosyalara kaydedildi.\n"
          ]
        }
      ],
      "source": [
        "import joblib\n",
        "\n",
        "# Model, Scaler ve Encoder'ı kaydetme\n",
        "joblib.dump(best_lr, 'logistic_regression_model.pkl')\n",
        "joblib.dump(scaler, 'scaler.pkl')\n",
        "joblib.dump(le, 'label_encoder.pkl')\n",
        "\n",
        "print(\"Model, scaler ve encoder başarıyla dosyalara kaydedildi.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuCKMgunnoiF"
      },
      "source": [
        "# Multi-layer Perceptron"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e845CWjC4ZUe"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# 1. Gerekli Kütüphanelerin İçe Aktarılması\n",
        "# -----------------------------\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, roc_auc_score, confusion_matrix,\n",
        "    matthews_corrcoef, f1_score, roc_curve, classification_report\n",
        ")\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.calibration import calibration_curve\n",
        "from scipy.stats import loguniform\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# -----------------------------\n",
        "# 2. Veri Yükleme ve Ön İşleme\n",
        "# -----------------------------\n",
        "\n",
        "# Veri yükleme\n",
        "df = pd.read_excel('df.xlsx')\n",
        "\n",
        "# Kategorik ve numerik değişkenleri ayırma\n",
        "categorical_features = ['Multifocal_PVC', 'Nonsustained_VT', 'gender', 'HTN', 'DM', 'Fullcompansasion']\n",
        "numeric_features = [\n",
        "    'pvc_percent', 'PVCQRS', 'EF', 'Age', 'PVC_Prematurity_index',\n",
        "    'QRS_ratio', 'mean_HR', 'symptom_duration', 'QTc_sinus',\n",
        "    'PVCCI_dispersion', 'CI_variability', 'PVC_Peak_QRS_duration',\n",
        "    'PVCCI', 'PVC_Compansatory_interval'\n",
        "]\n",
        "\n",
        "# Label Encoding için kategorik değişkenleri dönüştürme\n",
        "le = LabelEncoder()\n",
        "for col in categorical_features:\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "\n",
        "# Hedef değişkenin sınıflarını kontrol etme\n",
        "print(\"Train setindeki benzersiz sınıflar:\", df[df['region'] == 1]['Group'].unique())\n",
        "print(\"Test setindeki benzersiz sınıflar:\", df[df['region'] == 2]['Group'].unique())\n",
        "\n",
        "# Train ve test verilerini ayırma\n",
        "train_data = df[df['region'] == 1].copy()\n",
        "test_data = df[df['region'] == 2].copy()\n",
        "\n",
        "# Standartlaştırma\n",
        "scaler = StandardScaler()\n",
        "train_data[numeric_features] = scaler.fit_transform(train_data[numeric_features])\n",
        "test_data[numeric_features] = scaler.transform(test_data[numeric_features])\n",
        "\n",
        "# X ve y ayırma\n",
        "X_train = train_data[numeric_features + categorical_features]\n",
        "y_train = train_data['Group']\n",
        "X_test = test_data[numeric_features + categorical_features]\n",
        "y_test = test_data['Group']\n",
        "\n",
        "# -----------------------------\n",
        "# 3. Model Performansını Değerlendirmek İçin Fonksiyon\n",
        "# -----------------------------\n",
        "\n",
        "def evaluate_model(y_true, y_pred, y_pred_proba, model_name):\n",
        "    \"\"\"\n",
        "    Model performansını değerlendirmek için çeşitli metrikler hesaplar ve yazdırır.\n",
        "    \"\"\"\n",
        "    auc = roc_auc_score(y_true, y_pred_proba)\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    mcc = matthews_corrcoef(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "    print(f\"{model_name} Performans Metrikleri:\")\n",
        "    print(f\"AUC: {auc:.4f}\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(f\"Matthews Corr Coef: {mcc:.4f}\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_true, y_pred))\n",
        "\n",
        "    return auc\n",
        "\n",
        "# -----------------------------\n",
        "# 4. Hiperparametre Grid'i\n",
        "# -----------------------------\n",
        "\n",
        "param_grid = {\n",
        "    'hidden_layer_sizes': [\n",
        "        (50,), (100,), (50, 50), (100, 50), (100, 100),\n",
        "        (50, 25), (100, 50, 25), (100, 100, 50)\n",
        "    ],\n",
        "    'activation': ['relu', 'tanh'],\n",
        "    'solver': ['adam'],\n",
        "    'alpha': loguniform(1e-5, 1),\n",
        "    'learning_rate': ['constant', 'adaptive'],\n",
        "    'learning_rate_init': loguniform(1e-4, 1e-2),\n",
        "    'max_iter': [1000],\n",
        "    'early_stopping': [True],\n",
        "    'validation_fraction': [0.1],\n",
        "    'n_iter_no_change': [10],\n",
        "    'random_state': [42]\n",
        "}\n",
        "\n",
        "# -----------------------------\n",
        "# 5. RandomizedSearchCV ile En İyi Parametreleri Bulma\n",
        "# -----------------------------\n",
        "\n",
        "print(\"MLP modeli eğitiliyor...\")\n",
        "mlp = MLPClassifier()\n",
        "\n",
        "mlp_random = RandomizedSearchCV(\n",
        "    estimator=mlp,\n",
        "    param_distributions=param_grid,\n",
        "    n_iter=50,\n",
        "    cv=5,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "mlp_random.fit(X_train, y_train)\n",
        "\n",
        "# En iyi model ile tahmin\n",
        "best_mlp = mlp_random.best_estimator_\n",
        "y_pred_mlp = best_mlp.predict(X_test)\n",
        "# İkili sınıflandırma mı çok sınıflı mı kontrol et\n",
        "if best_mlp.predict_proba(X_test).shape[1] > 1:\n",
        "    # İkili sınıflandırma için, genellikle ikinci sütun sınıf 1 için\n",
        "    y_pred_proba_mlp = best_mlp.predict_proba(X_test)[:, 1]\n",
        "else:\n",
        "    # Tek sınıflı olasılık varsa\n",
        "    y_pred_proba_mlp = best_mlp.predict_proba(X_test)[:, 0]\n",
        "\n",
        "# -----------------------------\n",
        "# 6. Model Performansını Değerlendirme\n",
        "# -----------------------------\n",
        "\n",
        "auc_mlp = evaluate_model(y_test, y_pred_mlp, y_pred_proba_mlp, \"Multi-Layer Perceptron\")\n",
        "\n",
        "# -----------------------------\n",
        "# 7. Feature Importance Hesaplama (Permutation Importance Kullanarak)\n",
        "# -----------------------------\n",
        "\n",
        "perm_importance = permutation_importance(\n",
        "    best_mlp, X_test, y_test,\n",
        "    n_repeats=10, random_state=42, n_jobs=-1\n",
        ")\n",
        "\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': X_train.columns,\n",
        "    'importance': perm_importance.importances_mean\n",
        "})\n",
        "feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
        "\n",
        "# Feature Importance Grafiği\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='importance', y='feature', data=feature_importance)\n",
        "plt.title('MLP - Feature Importance (Permutation Importance)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# -----------------------------\n",
        "# 8. Learning Curves\n",
        "# -----------------------------\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# Loss Curve\n",
        "if hasattr(best_mlp, 'loss_curve_'):\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(best_mlp.loss_curve_)\n",
        "    plt.title('Loss Curve')\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.grid(True)\n",
        "\n",
        "# Validation Score Curve\n",
        "if hasattr(best_mlp, 'validation_scores_'):\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(best_mlp.validation_scores_)\n",
        "    plt.title('Validation Score Curve')\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('Score')\n",
        "    plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# -----------------------------\n",
        "# 9. Neural Network Architecture Visualization Fonksiyonu\n",
        "# -----------------------------\n",
        "\n",
        "def plot_neural_network(mlp):\n",
        "    \"\"\"\n",
        "    MLPClassifier modelinin sinir ağı mimarisini görselleştirir.\n",
        "    \"\"\"\n",
        "    n_layers = len(mlp.coefs_) + 1\n",
        "    layer_sizes = [mlp.coefs_[0].shape[0]] + [layer.shape[1] for layer in mlp.coefs_]\n",
        "\n",
        "    fig = plt.figure(figsize=(12, 8))\n",
        "    ax = fig.add_subplot(111)\n",
        "\n",
        "    # Plot nodes\n",
        "    for i, size in enumerate(layer_sizes):\n",
        "        x = np.full(size, i)\n",
        "        y = np.linspace(0, size-1, size)\n",
        "        ax.scatter(x, y, s=100, zorder=4, color='skyblue', edgecolors='k')\n",
        "\n",
        "        if i < len(layer_sizes)-1:\n",
        "            # Plot connections\n",
        "            for j in range(size):\n",
        "                for k in range(layer_sizes[i+1]):\n",
        "                    weight = mlp.coefs_[i][j, k]\n",
        "                    color = 'red' if weight < 0 else 'blue'\n",
        "                    alpha = min(abs(weight)*10, 1)  # Normalize alpha for better visibility\n",
        "                    ax.plot([i, i+1], [j, k], color=color, alpha=alpha, linewidth=0.5)\n",
        "\n",
        "    ax.set_xticks(range(n_layers))\n",
        "    ax.set_xticklabels(['Input'] + [f'Hidden {i}' for i in range(1, n_layers-1)] + ['Output'])\n",
        "    ax.set_yticks([])\n",
        "    plt.title('Neural Network Architecture')\n",
        "    plt.grid(False)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_neural_network(best_mlp)\n",
        "\n",
        "# -----------------------------\n",
        "# 10. Calibration Curve\n",
        "# -----------------------------\n",
        "\n",
        "prob_true, prob_pred = calibration_curve(y_test, y_pred_proba_mlp, n_bins=10)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(prob_pred, prob_true, marker='o', label='MLP')\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', label='Perfect Calibration')\n",
        "plt.xlabel('Mean Predicted Probability')\n",
        "plt.ylabel('True Probability')\n",
        "plt.title('Calibration Curve - MLP')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# -----------------------------\n",
        "# 11. Confusion Matrix ile Normalized Değerler\n",
        "# -----------------------------\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred_mlp)\n",
        "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_normalized, annot=True, cmap='Blues', fmt='.2f',\n",
        "            xticklabels=['Not Group', 'Group'],\n",
        "            yticklabels=['Not Group', 'Group'])\n",
        "plt.title('Normalized Confusion Matrix - MLP')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.show()\n",
        "\n",
        "# -----------------------------\n",
        "# 12. Decision Boundary Visualization Fonksiyonu\n",
        "# -----------------------------\n",
        "\n",
        "def plot_decision_boundary(model, X, y, feature1_idx=0, feature2_idx=1):\n",
        "    \"\"\"\n",
        "    İki özellik kullanarak modelin karar sınırını görselleştirir.\n",
        "    \"\"\"\n",
        "    # Sadece iki özelliği kullanarak karar sınırını görselleştiriyoruz\n",
        "    h = .02  # Mesh grid adım boyutu\n",
        "    x_min, x_max = X.iloc[:, feature1_idx].min() - 1, X.iloc[:, feature1_idx].max() + 1\n",
        "    y_min, y_max = X.iloc[:, feature2_idx].min() - 1, X.iloc[:, feature2_idx].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "\n",
        "    # Karar sınırını tahmin etmek için tüm grid noktalarını kullanıyoruz\n",
        "    X_plot = pd.DataFrame(np.zeros((xx.ravel().shape[0], X.shape[1])), columns=X.columns)\n",
        "    X_plot.iloc[:, feature1_idx] = xx.ravel()\n",
        "    X_plot.iloc[:, feature2_idx] = yy.ravel()\n",
        "\n",
        "    # Diğer özellikleri ortalama değerlerle dolduruyoruz\n",
        "    for i in range(X.shape[1]):\n",
        "        if i != feature1_idx and i != feature2_idx:\n",
        "            X_plot.iloc[:, i] = X.iloc[:, i].mean()\n",
        "\n",
        "    # Tahminler\n",
        "    Z = model.predict(X_plot)\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.contourf(xx, yy, Z, alpha=0.4, cmap='coolwarm')\n",
        "    scatter = plt.scatter(X.iloc[:, feature1_idx], X.iloc[:, feature2_idx], c=y, cmap='coolwarm', edgecolor='k', alpha=0.7)\n",
        "    plt.xlabel(X.columns[feature1_idx])\n",
        "    plt.ylabel(X.columns[feature2_idx])\n",
        "    plt.title('Decision Boundary - Top 2 Features')\n",
        "    plt.legend(*scatter.legend_elements(), title=\"Classes\")\n",
        "    plt.show()\n",
        "\n",
        "# -----------------------------\n",
        "# 13. Karar Sınırı Görselleştirme\n",
        "# -----------------------------\n",
        "\n",
        "# Top 2 Özelliği Kullanarak Decision Boundary Çizimi\n",
        "top_features = feature_importance.head(2)['feature'].values\n",
        "feature1_idx = X_train.columns.get_loc(top_features[0])\n",
        "feature2_idx = X_train.columns.get_loc(top_features[1])\n",
        "plot_decision_boundary(best_mlp, X_test, y_test.values, feature1_idx, feature2_idx)\n",
        "\n",
        "# -----------------------------\n",
        "# 14. En İyi Parametreleri Yazdırma\n",
        "# -----------------------------\n",
        "\n",
        "print(\"\\nEn İyi MLP Parametreleri:\")\n",
        "print(mlp_random.best_params_)\n",
        "\n",
        "# -----------------------------\n",
        "# 15. Model Complexity Analizi Fonksiyonu\n",
        "# -----------------------------\n",
        "\n",
        "def analyze_model_complexity(model, X_train, y_train, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Modelin karmaşıklığını ve eğitim ile test performansını analiz eder.\n",
        "    \"\"\"\n",
        "    n_layers = len(model.hidden_layer_sizes)\n",
        "    total_neurons = sum(model.hidden_layer_sizes)\n",
        "    n_parameters = sum(coef.size for coef in model.coefs_)\n",
        "\n",
        "    print(\"\\nModel Complexity Analysis:\")\n",
        "    print(f\"Number of hidden layers: {n_layers}\")\n",
        "    print(f\"Total number of neurons in hidden layers: {total_neurons}\")\n",
        "    print(f\"Total number of parameters: {n_parameters}\")\n",
        "\n",
        "    # Training vs test performance\n",
        "    train_score = model.score(X_train, y_train)\n",
        "    test_score = model.score(X_test, y_test)\n",
        "    print(f\"\\nTraining accuracy: {train_score:.3f}\")\n",
        "    print(f\"Test accuracy: {test_score:.3f}\")\n",
        "    print(f\"Overfitting gap: {train_score - test_score:.3f}\")\n",
        "\n",
        "analyze_model_complexity(best_mlp, X_train, y_train, X_test, y_test)\n",
        "\n",
        "# -----------------------------\n",
        "# 16. Sonuçları Kaydetme\n",
        "# -----------------------------\n",
        "\n",
        "results_mlp = pd.DataFrame({\n",
        "    'True_Labels': y_test,\n",
        "    'Predicted_Probabilities': y_pred_proba_mlp\n",
        "})\n",
        "results_mlp.to_csv('mlp_results.csv', index=False)\n",
        "print(\"\\nSonuçlar 'mlp_results.csv' dosyasına kaydedildi.\")\n",
        "\n",
        "# -----------------------------\n",
        "# 17. Prediction Confidence Analysis\n",
        "# -----------------------------\n",
        "\n",
        "prediction_confidence = np.max(best_mlp.predict_proba(X_test), axis=1)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.histplot(data=prediction_confidence, bins=30, kde=True)\n",
        "plt.title('Distribution of Prediction Confidence')\n",
        "plt.xlabel('Confidence')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n",
        "\n",
        "# -----------------------------\n",
        "# 18. SHAP Entegrasyonu\n",
        "# -----------------------------\n",
        "\n",
        "# SHAP değerlerini hesaplama\n",
        "print(\"\\nSHAP değerleri hesaplanıyor...\")\n",
        "\n",
        "# Hedef değişkenin sınıf sayısını kontrol etme\n",
        "n_classes = len(np.unique(y_train))\n",
        "print(f\"Hedef değişkenin sınıf sayısı: {n_classes}\")\n",
        "\n",
        "# SHAP hesaplama\n",
        "if n_classes == 2:\n",
        "    # İkili sınıflandırma için\n",
        "    explainer = shap.KernelExplainer(best_mlp.predict_proba, shap.sample(X_train, 100), link=\"logit\")\n",
        "    shap_values = explainer.shap_values(X_test, nsamples=100)\n",
        "\n",
        "    # SHAP Summary Plot (Bar)\n",
        "    print(\"SHAP Summary Plot oluşturuluyor...\")\n",
        "    if isinstance(shap_values, list):\n",
        "        # SHAP değerleri bir liste ise, ikinci sınıf için kullan\n",
        "        shap.summary_plot(shap_values[1], X_test, plot_type=\"bar\")  # Sınıf 1 için özet plot\n",
        "    else:\n",
        "        # SHAP değerleri tek bir dizi ise, üçüncü boyutta sınıfı seç\n",
        "        shap.summary_plot(shap_values[:, :, 1], X_test, plot_type=\"bar\")  # Sınıf 1 için özet plot\n",
        "\n",
        "    # SHAP Summary Plot (Scatter)\n",
        "    print(\"SHAP Scatter Summary Plot oluşturuluyor...\")\n",
        "    if isinstance(shap_values, list):\n",
        "        shap.summary_plot(shap_values[1], X_test)\n",
        "    else:\n",
        "        shap.summary_plot(shap_values[:, :, 1], X_test)\n",
        "\n",
        "    # SHAP Dependence Plot\n",
        "    top_feature = feature_importance.head(1)['feature'].values[0]\n",
        "    print(f\"{top_feature} için SHAP Dependence Plot oluşturuluyor...\")\n",
        "    if isinstance(shap_values, list):\n",
        "        shap.dependence_plot(top_feature, shap_values[1], X_test)\n",
        "    else:\n",
        "        shap.dependence_plot(top_feature, shap_values[:, :, 1], X_test)\n",
        "\n",
        "    # SHAP Force Plot (İlk birkaç örnek için)\n",
        "    print(\"SHAP Force Plot oluşturuluyor...\")\n",
        "    shap.initjs()\n",
        "    for i in range(min(3, X_test.shape[0])):  # İlk 3 örnek için\n",
        "        if isinstance(shap_values, list):\n",
        "            shap.force_plot(explainer.expected_value[1], shap_values[1][i], X_test.iloc[i], matplotlib=True)\n",
        "        else:\n",
        "            shap.force_plot(explainer.expected_value[1], shap_values[i, :, 1], X_test.iloc[i], matplotlib=True)\n",
        "        plt.show()\n",
        "else:\n",
        "    # Çok sınıflı sınıflandırma için\n",
        "    explainer = shap.KernelExplainer(best_mlp.predict_proba, shap.sample(X_train, 100), link=\"logit\")\n",
        "    shap_values = explainer.shap_values(X_test, nsamples=100)\n",
        "\n",
        "    for class_idx in range(n_classes):\n",
        "        print(f\"\\nSHAP Summary Plot oluşturuluyor - Sınıf {class_idx}:\")\n",
        "        if isinstance(shap_values, list):\n",
        "            shap.summary_plot(shap_values[class_idx], X_test, plot_type=\"bar\")\n",
        "        else:\n",
        "            shap.summary_plot(shap_values[:, :, class_idx], X_test, plot_type=\"bar\")\n",
        "        plt.title(f\"SHAP Summary Plot - Class {class_idx}\")\n",
        "        plt.show()\n",
        "\n",
        "        print(f\"SHAP Scatter Summary Plot oluşturuluyor - Sınıf {class_idx}:\")\n",
        "        if isinstance(shap_values, list):\n",
        "            shap.summary_plot(shap_values[class_idx], X_test)\n",
        "        else:\n",
        "            shap.summary_plot(shap_values[:, :, class_idx], X_test)\n",
        "        plt.title(f\"SHAP Scatter Summary Plot - Class {class_idx}\")\n",
        "        plt.show()\n",
        "\n",
        "        # SHAP Dependence Plot\n",
        "        top_feature = feature_importance.head(1)['feature'].values[0]\n",
        "        print(f\"{top_feature} için SHAP Dependence Plot oluşturuluyor - Sınıf {class_idx}:\")\n",
        "        if isinstance(shap_values, list):\n",
        "            shap.dependence_plot(top_feature, shap_values[class_idx], X_test)\n",
        "        else:\n",
        "            shap.dependence_plot(top_feature, shap_values[:, :, class_idx], X_test)\n",
        "        plt.title(f\"SHAP Dependence Plot - Class {class_idx}\")\n",
        "        plt.show()\n",
        "\n",
        "        # SHAP Force Plot (İlk birkaç örnek için)\n",
        "        print(f\"SHAP Force Plot oluşturuluyor - Sınıf {class_idx}:\")\n",
        "        shap.initjs()\n",
        "        for i in range(min(3, X_test.shape[0])):  # İlk 3 örnek için\n",
        "            if isinstance(shap_values, list):\n",
        "                shap.force_plot(explainer.expected_value[class_idx], shap_values[class_idx][i], X_test.iloc[i], matplotlib=True)\n",
        "            else:\n",
        "                shap.force_plot(explainer.expected_value[class_idx], shap_values[i, :, class_idx], X_test.iloc[i], matplotlib=True)\n",
        "            plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DXtcbuL0ZZp",
        "outputId": "cb773a31-21bf-4cc1-c794-9ba55c1ad6f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['scaler.pkl']"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Scaler'ı kaydet\n",
        "joblib.dump(scaler, 'scaler.pkl')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zwvf8KkoJ2M",
        "outputId": "4bbacb45-93cb-469e-b123-e3f95d153f22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model başarıyla mlp_model.pkl dosyasına kaydedildi.\n"
          ]
        }
      ],
      "source": [
        "import joblib\n",
        "\n",
        "# Modeli bir dosyaya kaydet\n",
        "joblib.dump(best_mlp, 'mlp_model.pkl')\n",
        "print(\"Model başarıyla mlp_model.pkl dosyasına kaydedildi.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtDJuCaVnrDl"
      },
      "source": [
        "# TabTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejupTUT1nwsM"
      },
      "outputs": [],
      "source": [
        "!pip install pytorch-lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOi5XMSbobzY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import EarlyStopping\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, confusion_matrix, classification_report, matthews_corrcoef\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Tabular dataset class\n",
        "class TabularDataset(Dataset):\n",
        "    def __init__(self, X, y=None):\n",
        "        self.X = torch.FloatTensor(X)\n",
        "        if y is not None:\n",
        "            self.y = torch.LongTensor(y)\n",
        "        else:\n",
        "            self.y = None\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.y is not None:\n",
        "            return self.X[idx], self.y[idx]\n",
        "        return self.X[idx]\n",
        "\n",
        "# TabTransformer model class\n",
        "class TabTransformer(pl.LightningModule):\n",
        "    def __init__(self, input_dim, num_classes=2, d_model=64, nhead=4, num_layers=3, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()  # Save the hyperparameters\n",
        "\n",
        "        self.embedding = nn.Linear(input_dim, d_model)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=d_model * 4,\n",
        "            dropout=dropout,\n",
        "            activation='gelu'\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_model // 2, num_classes)\n",
        "        )\n",
        "\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.validation_outputs = []  # Save outputs for validation\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = x.unsqueeze(0)  # Add sequence length dimension\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = x.squeeze(0)  # Remove sequence length dimension\n",
        "        return self.fc(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "        loss = self.criterion(y_hat, y)\n",
        "        self.log('train_loss', loss)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "        loss = self.criterion(y_hat, y)\n",
        "        self.log('val_loss', loss)\n",
        "        self.validation_outputs.append({'val_loss': loss, 'y_true': y, 'y_pred': torch.softmax(y_hat, dim=1)[:, 1]})\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        avg_loss = torch.stack([x['val_loss'] for x in self.validation_outputs]).mean()\n",
        "        y_true = torch.cat([x['y_true'] for x in self.validation_outputs])\n",
        "        y_pred = torch.cat([x['y_pred'] for x in self.validation_outputs])\n",
        "        auc = roc_auc_score(y_true.cpu(), y_pred.cpu())\n",
        "        self.log('val_auc', auc)\n",
        "        self.validation_outputs.clear()\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.AdamW(self.parameters(), lr=1e-4)\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
        "        return {\n",
        "            'optimizer': optimizer,\n",
        "            'lr_scheduler': scheduler,\n",
        "            'monitor': 'val_loss'\n",
        "        }\n",
        "\n",
        "def predict_proba(model, X):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        X_tensor = torch.FloatTensor(X.values)\n",
        "        logits = model(X_tensor)\n",
        "        probabilities = F.softmax(logits, dim=1)\n",
        "    return probabilities.cpu().numpy()\n",
        "\n",
        "# Model evaluation and metric calculation\n",
        "def evaluate_model(y_true, y_pred, y_pred_proba):\n",
        "    # Confusion matrix components\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    sensitivity = tp / (tp + fn)  # Recall\n",
        "    specificity = tn / (tn + fp)\n",
        "    ppv = tp / (tp + fp) if (tp + fp) > 0 else 0  # Positive Predictive Value\n",
        "    npv = tn / (tn + fn) if (tn + fn) > 0 else 0  # Negative Predictive Value\n",
        "    mcc = matthews_corrcoef(y_true, y_pred)\n",
        "\n",
        "    # Print metrics\n",
        "    print(\"\\nModel Performance Summary:\")\n",
        "    print(f\"Accuracy: {accuracy_score(y_true, y_pred):.3f}\")\n",
        "    print(f\"AUC: {roc_auc_score(y_true, y_pred_proba):.3f}\")\n",
        "    print(f\"F1 Score: {f1_score(y_true, y_pred):.3f}\")\n",
        "    print(f\"Sensitivity (Recall): {sensitivity:.3f}\")\n",
        "    print(f\"Specificity: {specificity:.3f}\")\n",
        "    print(f\"PPV (Precision): {ppv:.3f}\")\n",
        "    print(f\"NPV: {npv:.3f}\")\n",
        "    print(f\"MCC: {mcc:.3f}\")\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(confusion_matrix(y_true, y_pred))\n",
        "\n",
        "# Data preparation\n",
        "train_dataset = TabularDataset(X_train.values, y_train.values)\n",
        "test_dataset = TabularDataset(X_test.values, y_test.values)\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128)\n",
        "\n",
        "# Model training\n",
        "model = TabTransformer(\n",
        "    input_dim=X_train.shape[1],\n",
        "    d_model=64,\n",
        "    nhead=4,\n",
        "    num_layers=3,\n",
        "    dropout=0.1\n",
        ")\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=10,\n",
        "    mode='min'\n",
        ")\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    max_epochs=100,\n",
        "    accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "trainer.fit(model, train_loader, test_loader)\n",
        "\n",
        "# Model evaluation\n",
        "model.eval()\n",
        "y_pred_proba = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        x, _ = batch\n",
        "        outputs = model(x)\n",
        "        probas = torch.softmax(outputs, dim=1)[:, 1]\n",
        "        y_pred_proba.extend(probas.cpu().numpy())\n",
        "\n",
        "y_pred_proba = np.array(y_pred_proba)\n",
        "y_pred = (y_pred_proba > 0.5).astype(int)\n",
        "\n",
        "# Calculate and display all metrics\n",
        "evaluate_model(y_test, y_pred, y_pred_proba)\n",
        "\n",
        "# Display best hyperparameters\n",
        "print(\"\\nBest Hyperparameters:\")\n",
        "print(model.hparams)\n",
        "\n",
        "# Feature importance calculation and visualization\n",
        "def calculate_permutation_importance(model, X, y, n_repeats=10):\n",
        "    baseline_score = roc_auc_score(y, predict_proba(model, X)[:, 1])\n",
        "    importances = []\n",
        "\n",
        "    for column in X.columns:\n",
        "        scores = []\n",
        "        for _ in range(n_repeats):\n",
        "            X_permuted = X.copy()\n",
        "            X_permuted[column] = np.random.permutation(X_permuted[column])\n",
        "            score = roc_auc_score(y, predict_proba(model, X_permuted)[:, 1])\n",
        "            scores.append(baseline_score - score)\n",
        "        importances.append(np.mean(scores))\n",
        "\n",
        "    return importances\n",
        "\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': X_train.columns,\n",
        "    'importance': calculate_permutation_importance(model, X_test, y_test)\n",
        "})\n",
        "feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='importance', y='feature', data=feature_importance)\n",
        "plt.title('TabTransformer - Feature Importance (Permutation Importance)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Saving results\n",
        "results_tab = pd.DataFrame({\n",
        "    'True_Labels': y_test,\n",
        "    'Predicted_Probabilities': y_pred_proba\n",
        "})\n",
        "results_tab.to_csv('tabtransformer_results.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFrHmQm3ogCw",
        "outputId": "d1d4fd79-b181-471f-a0eb-5b3db6f995b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model başarıyla tabtransformer_model.pth dosyasına kaydedildi.\n",
            "Scaler başarıyla scaler.pkl dosyasına kaydedildi.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import pickle\n",
        "\n",
        "# Modelinizi kaydetmek\n",
        "torch.save(model.state_dict(), 'tabtransformer_model.pth')\n",
        "print(\"Model başarıyla tabtransformer_model.pth dosyasına kaydedildi.\")\n",
        "\n",
        "# Ölçekleyiciyi kaydetmek\n",
        "with open('scaler.pkl', 'wb') as scaler_file:\n",
        "    pickle.dump(scaler, scaler_file)\n",
        "print(\"Scaler başarıyla scaler.pkl dosyasına kaydedildi.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqV_yJtclBV9"
      },
      "outputs": [],
      "source": [
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')  # Set the backend to Agg for better file saving\n",
        "\n",
        "def predict_proba_for_shap(X):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        if isinstance(X, pd.DataFrame):\n",
        "            X_tensor = torch.FloatTensor(X.values)\n",
        "        elif isinstance(X, np.ndarray):\n",
        "            X_tensor = torch.FloatTensor(X)\n",
        "        else:\n",
        "            raise TypeError(\"Input must be either pandas DataFrame or numpy array\")\n",
        "\n",
        "        logits = model(X_tensor)\n",
        "        probabilities = torch.softmax(logits, dim=1)\n",
        "        return probabilities[:, 1].cpu().numpy()\n",
        "\n",
        "# Create background dataset\n",
        "n_background = 50\n",
        "background = X_train.sample(n=n_background, random_state=42)\n",
        "\n",
        "print(\"Creating SHAP explainer...\")\n",
        "explainer = shap.KernelExplainer(\n",
        "    predict_proba_for_shap,\n",
        "    background,\n",
        "    link=\"logit\",\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Sample test data\n",
        "n_explain = 50\n",
        "X_sample = X_test.sample(n=n_explain, random_state=42)\n",
        "\n",
        "print(\"Calculating SHAP values...\")\n",
        "shap_values = explainer.shap_values(\n",
        "    X_sample,\n",
        "    nsamples=100,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "print(\"Generating SHAP plots...\")\n",
        "def save_shap_plots():\n",
        "    try:\n",
        "        # Summary Dot Plot\n",
        "        plt.clf()  # Clear the current figure\n",
        "        fig_dot = plt.figure(figsize=(12, 8))\n",
        "        shap.summary_plot(\n",
        "            shap_values,\n",
        "            X_sample,\n",
        "            plot_type=\"dot\",\n",
        "            show=False,\n",
        "            max_display=20\n",
        "        )\n",
        "        plt.title('SHAP Summary Plot (Dot)')\n",
        "        plt.tight_layout()\n",
        "        fig_dot.savefig('shap_summary_dot.png', bbox_inches='tight', dpi=300)\n",
        "        plt.close(fig_dot)\n",
        "\n",
        "        # Summary Bar Plot\n",
        "        plt.clf()\n",
        "        fig_bar = plt.figure(figsize=(12, 8))\n",
        "        shap.summary_plot(\n",
        "            shap_values,\n",
        "            X_sample,\n",
        "            plot_type=\"bar\",\n",
        "            show=False,\n",
        "            max_display=20\n",
        "        )\n",
        "        plt.title('SHAP Summary Plot (Bar)')\n",
        "        plt.tight_layout()\n",
        "        fig_bar.savefig('shap_summary_bar.png', bbox_inches='tight', dpi=300)\n",
        "        plt.close(fig_bar)\n",
        "\n",
        "        # Feature importance calculation\n",
        "        feature_importance = np.abs(shap_values).mean(0)\n",
        "        most_important_idx = np.argmax(feature_importance)\n",
        "        important_feature = X_test.columns[most_important_idx]\n",
        "\n",
        "        # Dependence Plot\n",
        "        plt.clf()\n",
        "        fig_dep = plt.figure(figsize=(12, 8))\n",
        "        shap.dependence_plot(\n",
        "            most_important_idx,\n",
        "            shap_values,\n",
        "            X_sample,\n",
        "            show=False\n",
        "        )\n",
        "        plt.title(f'SHAP Dependence Plot for {important_feature}')\n",
        "        plt.tight_layout()\n",
        "        fig_dep.savefig(f'shap_dependence_{important_feature}.png', bbox_inches='tight', dpi=300)\n",
        "        plt.close(fig_dep)\n",
        "\n",
        "        # Force Plot\n",
        "        plt.clf()\n",
        "        fig_force = plt.figure(figsize=(15, 3))\n",
        "        shap.force_plot(\n",
        "            explainer.expected_value,\n",
        "            shap_values[0],\n",
        "            X_sample.iloc[0],\n",
        "            show=False,\n",
        "            matplotlib=True\n",
        "        )\n",
        "        plt.title('SHAP Force Plot (Single Example)')\n",
        "        plt.tight_layout()\n",
        "        fig_force.savefig('shap_force_plot.png', bbox_inches='tight', dpi=300)\n",
        "        plt.close(fig_force)\n",
        "\n",
        "        # Save feature importance DataFrame\n",
        "        shap_importance = pd.DataFrame({\n",
        "            'feature': X_test.columns,\n",
        "            'importance': feature_importance\n",
        "        })\n",
        "        shap_importance = shap_importance.sort_values('importance', ascending=False)\n",
        "        shap_importance.to_csv('shap_importance.csv', index=False)\n",
        "\n",
        "        print(\"\\nFeature Importance based on SHAP values:\")\n",
        "        print(shap_importance)\n",
        "\n",
        "        # Create a bar plot of feature importance\n",
        "        plt.clf()\n",
        "        fig_imp = plt.figure(figsize=(12, 8))\n",
        "        plt.bar(range(len(feature_importance)), shap_importance['importance'])\n",
        "        plt.xticks(range(len(feature_importance)), shap_importance['feature'], rotation=45, ha='right')\n",
        "        plt.title('Feature Importance from SHAP Values')\n",
        "        plt.xlabel('Features')\n",
        "        plt.ylabel('Mean |SHAP Value|')\n",
        "        plt.tight_layout()\n",
        "        fig_imp.savefig('feature_importance_bar.png', bbox_inches='tight', dpi=300)\n",
        "        plt.close(fig_imp)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating plots: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Execute SHAP analysis\n",
        "try:\n",
        "    save_shap_plots()\n",
        "    print(\"\\nSHAP analysis completed successfully. The following files have been saved:\")\n",
        "    print(\"- shap_summary_dot.png\")\n",
        "    print(\"- shap_summary_bar.png\")\n",
        "    print(f\"- shap_dependence_{X_test.columns[np.argmax(np.abs(shap_values).mean(0))]}.png\")\n",
        "    print(\"- shap_force_plot.png\")\n",
        "    print(\"- feature_importance_bar.png\")\n",
        "    print(\"- shap_importance.csv\")\n",
        "    print(\"- shap_values.csv\")\n",
        "except Exception as e:\n",
        "    print(f\"SHAP analysis failed: {str(e)}\")\n",
        "\n",
        "# Save SHAP values\n",
        "shap_results = pd.DataFrame(\n",
        "    shap_values,\n",
        "    columns=X_test.columns\n",
        ")\n",
        "shap_results.to_csv('shap_values.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jgkImD4nxBR"
      },
      "source": [
        "# TabNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mv9zaekP6VW2"
      },
      "outputs": [],
      "source": [
        "pip install pytorch-tabnet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rh8CUDbu2wsE"
      },
      "outputs": [],
      "source": [
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PcbDJ0hezWwy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from pytorch_tabnet.tab_model import TabNetClassifier\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.calibration import calibration_curve\n",
        "import optuna\n",
        "\n",
        "# Assuming the data is already loaded and preprocessed as X and y\n",
        "# Split data into train and test sets\n",
        "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define a function to evaluate the model\n",
        "def evaluate_model(y_true, y_pred, y_pred_proba, model_name):\n",
        "    auc = roc_auc_score(y_true, y_pred_proba)\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    print(f\"{model_name} - AUC: {auc:.4f}, Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}\")\n",
        "    return auc, accuracy, f1\n",
        "\n",
        "# Optuna objective function\n",
        "def objective(trial):\n",
        "    params = {\n",
        "        \"n_d\": trial.suggest_int(\"n_d\", 8, 64, step=8),\n",
        "        \"n_a\": trial.suggest_int(\"n_a\", 8, 64, step=8),\n",
        "        \"n_steps\": trial.suggest_int(\"n_steps\", 3, 10),\n",
        "        \"gamma\": trial.suggest_float(\"gamma\", 1.0, 2.0),\n",
        "        \"lambda_sparse\": trial.suggest_float(\"lambda_sparse\", 1e-6, 1e-3, log=True),\n",
        "        \"optimizer_fn\": torch.optim.Adam,\n",
        "        \"optimizer_params\": dict(lr=trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)),\n",
        "        \"mask_type\": trial.suggest_categorical(\"mask_type\", [\"entmax\", \"sparsemax\"]),\n",
        "        \"n_shared\": trial.suggest_int(\"n_shared\", 1, 3),\n",
        "        \"n_independent\": trial.suggest_int(\"n_independent\", 1, 3),\n",
        "        \"scheduler_params\": dict(\n",
        "            mode=\"min\",\n",
        "            patience=trial.suggest_int(\"patienceScheduler\", 3, 10),\n",
        "            min_lr=1e-5,\n",
        "            factor=0.5,\n",
        "        ),\n",
        "        \"scheduler_fn\": torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
        "        \"verbose\": 0\n",
        "    }\n",
        "\n",
        "    model = TabNetClassifier(**params)\n",
        "    model.fit(\n",
        "        X_train=X_train.values,\n",
        "        y_train=y_train.values,\n",
        "        eval_set=[(X_test.values, y_test.values)],\n",
        "        eval_name=[\"valid\"],\n",
        "        patience=trial.suggest_int(\"patience\", 10, 30),\n",
        "        max_epochs=trial.suggest_int(\"epochs\", 10, 100),\n",
        "        batch_size=128,\n",
        "        virtual_batch_size=64,\n",
        "    )\n",
        "    y_pred_proba = model.predict_proba(X_test.values)[:, 1]\n",
        "    auc = roc_auc_score(y_test, y_pred_proba)\n",
        "    return -auc  # Negative because Optuna minimizes\n",
        "\n",
        "# Run Optuna optimization\n",
        "study = optuna.create_study(direction=\"minimize\")\n",
        "study.optimize(objective, n_trials=50, timeout=3600)\n",
        "\n",
        "# Display best parameters\n",
        "print(\"Best parameters found:\", study.best_params)\n",
        "\n",
        "# Train the model with the best parameters\n",
        "best_params = study.best_params\n",
        "final_params = {\n",
        "    \"n_d\": best_params[\"n_d\"],\n",
        "    \"n_a\": best_params[\"n_a\"],\n",
        "    \"n_steps\": best_params[\"n_steps\"],\n",
        "    \"gamma\": best_params[\"gamma\"],\n",
        "    \"lambda_sparse\": best_params[\"lambda_sparse\"],\n",
        "    \"optimizer_fn\": torch.optim.Adam,\n",
        "    \"optimizer_params\": dict(lr=best_params[\"lr\"]),\n",
        "    \"mask_type\": best_params[\"mask_type\"],\n",
        "    \"n_shared\": best_params[\"n_shared\"],\n",
        "    \"n_independent\": best_params[\"n_independent\"],\n",
        "    \"scheduler_params\": dict(\n",
        "        mode=\"min\",\n",
        "        patience=best_params[\"patienceScheduler\"],\n",
        "        min_lr=1e-5,\n",
        "        factor=0.5,\n",
        "    ),\n",
        "    \"scheduler_fn\": torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
        "    \"verbose\": 1\n",
        "}\n",
        "\n",
        "clf = TabNetClassifier(**final_params)\n",
        "clf.fit(\n",
        "    X_train=X_train.values,\n",
        "    y_train=y_train.values,\n",
        "    eval_set=[(X_test.values, y_test.values)],\n",
        "    eval_name=[\"valid\"],\n",
        "    patience=best_params[\"patience\"],\n",
        "    max_epochs=best_params[\"epochs\"],\n",
        "    batch_size=128,\n",
        "    virtual_batch_size=64,\n",
        ")\n",
        "\n",
        "# Predictions\n",
        "y_pred = clf.predict(X_test.values)\n",
        "y_pred_proba = clf.predict_proba(X_test.values)[:, 1]\n",
        "\n",
        "# Model evaluation\n",
        "evaluate_model(y_test, y_pred, y_pred_proba, \"TabNet with Optuna\")\n",
        "\n",
        "# Feature importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': X_train.columns,\n",
        "    'importance': clf.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(\"\\nFeature Importance:\")\n",
        "print(feature_importance)\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='importance', y='feature', data=feature_importance)\n",
        "plt.title('TabNet - Feature Importance')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calibration curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "prob_true, prob_pred = calibration_curve(y_test, y_pred_proba, n_bins=10)\n",
        "plt.plot(prob_pred, prob_true, marker='o', label='TabNet')\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', label='Perfectly calibrated')\n",
        "plt.xlabel('Mean predicted probability')\n",
        "plt.ylabel('True probability')\n",
        "plt.title('Calibration Curve - TabNet')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Save results\n",
        "results_tabnet = pd.DataFrame({\n",
        "    'True_Labels': y_test,\n",
        "    'Predicted_Probabilities': y_pred_proba\n",
        "})\n",
        "results_tabnet.to_csv('tabnet_results.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ilaKMfIf6Bg"
      },
      "outputs": [],
      "source": [
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# StandardScaler'ı başlatın\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Eğitim verisini ölçeklendirin ve scaler'ı eğitin\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "# Test verisini ölçeklendirin (Scaler'ı yeniden eğitmeyin)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Hedef değişkenin sınıf sayısını kontrol etme\n",
        "n_classes = len(np.unique(y_train))\n",
        "print(f\"Hedef değişkenin sınıf sayısı: {n_classes}\")\n",
        "\n",
        "# Model tahmin fonksiyonunu tanımlayın\n",
        "def model_predict_shap(X):\n",
        "    return clf.predict_proba(X)\n",
        "\n",
        "# SHAP için arka plan verisi seçin (genellikle eğitim verisinin bir alt kümesi)\n",
        "background = shap.sample(X_train_scaled, 100, random_state=42)\n",
        "\n",
        "# SHAP KernelExplainer oluşturun\n",
        "explainer = shap.KernelExplainer(model_predict_shap, background)\n",
        "\n",
        "# SHAP değerlerini hesaplayın (bu işlem zaman alabilir)\n",
        "print(\"SHAP değerleri hesaplanıyor, lütfen bekleyin...\")\n",
        "shap_values = explainer.shap_values(X_test_scaled, nsamples=100)\n",
        "\n",
        "# SHAP Summary Plot (Bar)\n",
        "print(\"SHAP Summary Plot oluşturuluyor...\")\n",
        "if n_classes == 2:\n",
        "    # İkili sınıflandırma için\n",
        "    if isinstance(shap_values, list):\n",
        "        # SHAP değerleri bir liste ise, ikinci sınıf için kullan\n",
        "        shap.summary_plot(shap_values[1], X_test_scaled, feature_names=X_train.columns, plot_type=\"bar\")\n",
        "    else:\n",
        "        # SHAP değerleri tek bir dizi ise, üçüncü boyutta sınıfı seç\n",
        "        shap.summary_plot(shap_values[:, :, 1], X_test_scaled, feature_names=X_train.columns, plot_type=\"bar\")\n",
        "else:\n",
        "    # Çok sınıflı sınıflandırma için\n",
        "    for class_idx in range(n_classes):\n",
        "        print(f\"\\nSHAP Summary Plot oluşturuluyor - Sınıf {class_idx}:\")\n",
        "        if isinstance(shap_values, list):\n",
        "            shap.summary_plot(shap_values[class_idx], X_test_scaled, feature_names=X_train.columns, plot_type=\"bar\")\n",
        "        else:\n",
        "            shap.summary_plot(shap_values[:, :, class_idx], X_test_scaled, feature_names=X_train.columns, plot_type=\"bar\")\n",
        "        plt.title(f\"SHAP Summary Plot - Class {class_idx}\")\n",
        "        plt.show()\n",
        "\n",
        "# SHAP Summary Plot (Scatter)\n",
        "print(\"SHAP Scatter Summary Plot oluşturuluyor...\")\n",
        "if n_classes == 2:\n",
        "    if isinstance(shap_values, list):\n",
        "        shap.summary_plot(shap_values[1], X_test_scaled, feature_names=X_train.columns)\n",
        "    else:\n",
        "        shap.summary_plot(shap_values[:, :, 1], X_test_scaled, feature_names=X_train.columns)\n",
        "else:\n",
        "    for class_idx in range(n_classes):\n",
        "        shap.summary_plot(shap_values[class_idx], X_test_scaled, feature_names=X_train.columns)\n",
        "        plt.title(f\"SHAP Scatter Summary Plot - Class {class_idx}\")\n",
        "        plt.show()\n",
        "\n",
        "# SHAP Dependence Plot\n",
        "if n_classes == 2:\n",
        "    top_feature = feature_importance.head(1)['feature'].values[0]\n",
        "    print(f\"{top_feature} için SHAP Dependence Plot oluşturuluyor...\")\n",
        "    if isinstance(shap_values, list):\n",
        "        shap.dependence_plot(top_feature, shap_values[1], X_test_scaled, feature_names=X_train.columns)\n",
        "    else:\n",
        "        shap.dependence_plot(top_feature, shap_values[:, :, 1], X_test_scaled, feature_names=X_train.columns)\n",
        "    plt.title(f\"SHAP Dependence Plot - {top_feature}\")\n",
        "    plt.show()\n",
        "else:\n",
        "    for class_idx in range(n_classes):\n",
        "        top_feature = feature_importance.head(1)['feature'].values[0]\n",
        "        print(f\"{top_feature} için SHAP Dependence Plot oluşturuluyor - Sınıf {class_idx}:\")\n",
        "        if isinstance(shap_values, list):\n",
        "            shap.dependence_plot(top_feature, shap_values[class_idx], X_test_scaled, feature_names=X_train.columns)\n",
        "        else:\n",
        "            shap.dependence_plot(top_feature, shap_values[:, :, class_idx], X_test_scaled, feature_names=X_train.columns)\n",
        "        plt.title(f\"SHAP Dependence Plot - Class {class_idx}\")\n",
        "        plt.show()\n",
        "\n",
        "# SHAP Force Plot (İlk birkaç örnek için)\n",
        "print(\"SHAP Force Plot oluşturuluyor...\")\n",
        "shap.initjs()\n",
        "for i in range(min(3, X_test_scaled.shape[0])):  # İlk 3 örnek için\n",
        "    if n_classes == 2:\n",
        "        if isinstance(shap_values, list):\n",
        "            shap.force_plot(explainer.expected_value[1], shap_values[1][i], X_test_scaled[i], matplotlib=True)\n",
        "        else:\n",
        "            shap.force_plot(explainer.expected_value[1], shap_values[i, :, 1], X_test_scaled[i], matplotlib=True)\n",
        "    else:\n",
        "        for class_idx in range(n_classes):\n",
        "            if isinstance(shap_values, list):\n",
        "                shap.force_plot(explainer.expected_value[class_idx], shap_values[class_idx][i], X_test_scaled[i], matplotlib=True)\n",
        "            else:\n",
        "                shap.force_plot(explainer.expected_value[class_idx], shap_values[i, :, class_idx], X_test_scaled[i], matplotlib=True)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlqXzisN-E9O",
        "outputId": "ead6bd4d-16a7-4691-efbc-b3afbc8369a0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['scaler.joblib']"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import joblib\n",
        "\n",
        "# Scaler'ı kaydedin\n",
        "joblib.dump(scaler, 'scaler.joblib')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-L6QE7F-PIQ"
      },
      "outputs": [],
      "source": [
        "# Tüm modeli kaydedin\n",
        "torch.save(clf, 'tabnet_model.pth')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1retHtdLn1Nl"
      },
      "source": [
        "# KAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLkBpfq2n3gp"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, classification_report\n",
        "from sklearn.metrics import matthews_corrcoef, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "#import shap\n",
        "\n",
        "# Veri yükleme\n",
        "df = pd.read_excel('/content/df.xlsx')\n",
        "\n",
        "# Train ve test verilerini ayırma\n",
        "train_data = df[df['region'] == 1].copy()\n",
        "test_data = df[df['region'] == 2].copy()\n",
        "\n",
        "# Kategorik ve numerik değişkenleri ayırma\n",
        "categorical_features = ['Multifocal_PVC', 'Nonsustained_VT', 'gender', 'HTN', 'DM', 'Fullcompansasion']\n",
        "numeric_features = ['pvc_percent', 'PVCQRS', 'EF', 'Age', 'PVC_Prematurity_index', 'QRS_ratio',\n",
        "                   'mean_HR', 'symptom_duration', 'QTc_sinus', 'PVCCI_dispersion', 'CI_variability',\n",
        "                   'PVC_Peak_QRS_duration', 'PVCCI', 'PVC_Compansatory_interval']\n",
        "\n",
        "# Label Encoding için kategorik değişkenleri dönüştürme\n",
        "le = LabelEncoder()\n",
        "for col in categorical_features:\n",
        "    train_data[col] = le.fit_transform(train_data[col])\n",
        "    test_data[col] = le.transform(test_data[col])\n",
        "\n",
        "# Standartlaştırma\n",
        "scaler = StandardScaler()\n",
        "train_data[numeric_features] = scaler.fit_transform(train_data[numeric_features])\n",
        "test_data[numeric_features] = scaler.transform(test_data[numeric_features])\n",
        "\n",
        "# X ve y ayırma\n",
        "X_train = train_data[numeric_features + categorical_features]\n",
        "y_train = train_data['Group']\n",
        "X_test = test_data[numeric_features + categorical_features]\n",
        "y_test = test_data['Group']\n",
        "\n",
        "def evaluate_model(y_true, y_pred, y_pred_proba, model_name):\n",
        "    \"\"\"\n",
        "    Model performans metriklerini hesaplar ve yazdırır\n",
        "    \"\"\"\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    auc = roc_auc_score(y_true, y_pred_proba)\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    sensitivity = tp / (tp + fn)\n",
        "    specificity = tn / (tn + fp)\n",
        "    ppv = tp / (tp + fp)\n",
        "    npv = tn / (tn + fn)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    mcc = matthews_corrcoef(y_true, y_pred)\n",
        "\n",
        "    print(f\"\\n{model_name} Performans Metrikleri:\")\n",
        "    print(f\"Accuracy: {accuracy:.3f}\")\n",
        "    print(f\"AUC: {auc:.3f}\")\n",
        "    print(f\"Sensitivity: {sensitivity:.3f}\")\n",
        "    print(f\"Specificity: {specificity:.3f}\")\n",
        "    print(f\"PPV: {ppv:.3f}\")\n",
        "    print(f\"NPV: {npv:.3f}\")\n",
        "    print(f\"F1 Score: {f1:.3f}\")\n",
        "    print(f\"MCC: {mcc:.3f}\")\n",
        "\n",
        "    return auc\n",
        "\n",
        "def plot_roc_curves(models_dict):\n",
        "    \"\"\"\n",
        "    Tüm modeller için ROC eğrilerini çizer\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    for model_name, (model, y_pred_proba) in models_dict.items():\n",
        "        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
        "        auc = roc_auc_score(y_test, y_pred_proba)\n",
        "        plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc:.3f})')\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC Curves for All Models')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28xhdxHrkoOU"
      },
      "outputs": [],
      "source": [
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'scaler': scaler,\n",
        "    'feature_names': X_train.columns.tolist(),\n",
        "    'X_train_scaled': X_train_scaled,\n",
        "    'X_test_scaled': X_test_scaled,\n",
        "    'y_test': y_test\n",
        "}, 'kan_model_full.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGHZo0Mcktid"
      },
      "outputs": [],
      "source": [
        "# Eğitim sonrası model kaydetme\n",
        "X_train_np = X_train_scaled\n",
        "X_test_np = X_test_scaled\n",
        "\n",
        "if isinstance(X_train_np, np.ndarray):\n",
        "    X_train_np = X_train_np.astype(np.float32)\n",
        "if isinstance(X_test_np, np.ndarray):\n",
        "    X_test_np = X_test_np.astype(np.float32)\n",
        "\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'feature_names': X_train.columns.tolist(),\n",
        "    'X_train': X_train_np,\n",
        "    'X_test': X_test_np\n",
        "}, 'kan_model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKnsu3Z-ktaD"
      },
      "outputs": [],
      "source": [
        "def analyze_shap(model_path):\n",
        "    import torch\n",
        "    import shap\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    # Define or import KANWrapper if not already defined\n",
        "    class KANWrapper:\n",
        "        def __init__(self, model, device):\n",
        "            self.model = model\n",
        "            self.device = device\n",
        "\n",
        "        def __call__(self, data):\n",
        "            self.model.eval()\n",
        "            with torch.no_grad():\n",
        "                data = torch.FloatTensor(data).to(self.device)\n",
        "                outputs = self.model(data)\n",
        "                probabilities = torch.softmax(outputs, dim=1)[:, 1].cpu().numpy()\n",
        "            return probabilities\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Load the checkpoint with map_location to ensure compatibility\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "\n",
        "    # Initialize the model architecture\n",
        "    input_dim = len(checkpoint['feature_names'])\n",
        "    hidden_dims = [64, 32]\n",
        "    inner_dim = 32\n",
        "    model = KolmogorovArnoldNetwork(input_dim, hidden_dims, inner_dim).to(device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.eval()\n",
        "\n",
        "    # Prepare background and explain data\n",
        "    background_data = checkpoint['X_train'][:100]\n",
        "    explain_data = checkpoint['X_test'][:100]\n",
        "\n",
        "    # Convert feature_names to a NumPy array\n",
        "    feature_names = np.array(checkpoint['feature_names'])\n",
        "\n",
        "    # Initialize SHAP KernelExplainer\n",
        "    wrapper = KANWrapper(model, device)\n",
        "    explainer = shap.KernelExplainer(wrapper, background_data)\n",
        "\n",
        "    # Calculate SHAP values\n",
        "    shap_values = explainer.shap_values(explain_data)\n",
        "\n",
        "    # Handle multi-class or binary classification\n",
        "    if isinstance(shap_values, list):\n",
        "        shap_values_plot = shap_values[1]\n",
        "    else:\n",
        "        shap_values_plot = shap_values\n",
        "\n",
        "    # Visualization: SHAP Summary Plot\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    shap.summary_plot(\n",
        "        shap_values_plot,\n",
        "        explain_data,\n",
        "        feature_names=feature_names,\n",
        "        show=False\n",
        "    )\n",
        "\n",
        "    # Feature Importance DataFrame\n",
        "    feature_importance = np.abs(shap_values_plot).mean(axis=0)\n",
        "    importance_df = pd.DataFrame({\n",
        "        'feature': feature_names,\n",
        "        'importance': feature_importance\n",
        "    }).sort_values('importance', ascending=False)\n",
        "\n",
        "    # Save the summary plot\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('shap_summary.png')\n",
        "    plt.close()\n",
        "\n",
        "    return shap_values, importance_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnRj20M_k28W"
      },
      "outputs": [],
      "source": [
        "shap_values, importance_df = analyze_shap('kan_model.pth')\n",
        "\n",
        "# Display feature importance\n",
        "print(importance_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-d5XcpK5-V-"
      },
      "source": [
        "# Kolmogorov Arnold Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGvpY_3P5k7d"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "import torch.nn as nn\n",
        "# GPU kullanılabilirse GPU'yu, değilse CPU'yu kullan\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "class KolmogorovArnoldNetworkWithDropout(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims=[64, 32], inner_dim=32, dropout_rate=0.2):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "\n",
        "        # KAN layers with dropout\n",
        "        layers = []\n",
        "        prev_dim = input_dim\n",
        "        for hidden_dim in hidden_dims:\n",
        "            layers.append(KolmogorovArnoldLayer(prev_dim, inner_dim, hidden_dim))\n",
        "            layers.append(nn.Dropout(dropout_rate))\n",
        "            prev_dim = hidden_dim\n",
        "\n",
        "        self.kan_layers = nn.ModuleList(layers)\n",
        "        self.output_layer = nn.Linear(hidden_dims[-1], 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.kan_layers:\n",
        "            x = layer(x)\n",
        "        return self.output_layer(x)\n",
        "\n",
        "def objective(trial):\n",
        "    # Hiperparametreleri tanımla\n",
        "    hidden_dims = [\n",
        "        trial.suggest_int('hidden_dim1', 32, 256),\n",
        "        trial.suggest_int('hidden_dim2', 16, 128)\n",
        "    ]\n",
        "    inner_dim = trial.suggest_int('inner_dim', 16, 128)\n",
        "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
        "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
        "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128])\n",
        "\n",
        "    # Dataset ve DataLoader oluştur\n",
        "    train_dataset = KANDataset(X_train_scaled, y_train.values)\n",
        "    valid_dataset = KANDataset(X_test_scaled, y_test.values)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size)\n",
        "\n",
        "    # Model oluştur\n",
        "    model = KolmogorovArnoldNetworkWithDropout(\n",
        "        input_dim=X_train.shape[1],\n",
        "        hidden_dims=hidden_dims,\n",
        "        inner_dim=inner_dim,\n",
        "        dropout_rate=dropout_rate\n",
        "    ).to(device)\n",
        "\n",
        "    # Optimizer ve loss function\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Early stopping parametreleri\n",
        "    patience = 5\n",
        "    best_auc = 0\n",
        "    patience_counter = 0\n",
        "\n",
        "    # Eğitim döngüsü\n",
        "    for epoch in range(30):  # Maximum 30 epoch\n",
        "        # Train\n",
        "        model.train()\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        all_probs = []\n",
        "        all_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in valid_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                probs = torch.softmax(outputs, dim=1)\n",
        "                all_probs.extend(probs[:, 1].cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        current_auc = roc_auc_score(all_labels, all_probs)\n",
        "\n",
        "        # Early stopping kontrolü\n",
        "        if current_auc > best_auc:\n",
        "            best_auc = current_auc\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            break\n",
        "\n",
        "    return best_auc\n",
        "\n",
        "# Optuna çalıştırma\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=10)\n",
        "\n",
        "# En iyi hiperparametreler\n",
        "print(\"En iyi hiperparametreler:\")\n",
        "print(study.best_params)\n",
        "print(f\"\\nEn iyi AUC skoru: {study.best_value:.4f}\")\n",
        "\n",
        "# Hiperparametre önem analizi\n",
        "importance = optuna.importance.get_param_importances(study)\n",
        "print(\"\\nHiperparametre önem sıralaması:\")\n",
        "for param, score in importance.items():\n",
        "    print(f\"{param}: {score:.4f}\")\n",
        "\n",
        "# En iyi modeli eğit\n",
        "best_params = study.best_params\n",
        "final_model = KolmogorovArnoldNetworkWithDropout(\n",
        "    input_dim=X_train.shape[1],\n",
        "    hidden_dims=[best_params['hidden_dim1'], best_params['hidden_dim2']],\n",
        "    inner_dim=best_params['inner_dim'],\n",
        "    dropout_rate=best_params['dropout_rate']\n",
        ").to(device)\n",
        "\n",
        "# Final modeli en iyi hiperparametrelerle eğit\n",
        "train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=best_params['batch_size'])\n",
        "optimizer = optim.Adam(final_model.parameters(), lr=best_params['learning_rate'])\n",
        "\n",
        "# Model eğitimi ve değerlendirmesi\n",
        "train_model(final_model, train_loader, criterion, optimizer, device)\n",
        "_, _, y_pred_proba, y_true = evaluate_model(final_model, test_loader, criterion, device)\n",
        "y_pred = (y_pred_proba > 0.5).astype(int)\n",
        "\n",
        "# Final metrikleri hesapla\n",
        "print(\"\\nOptimize edilmiş model performansı:\")\n",
        "calculate_metrics(y_true, y_pred, y_pred_proba, \"Optimized KAN\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1BTtBGrFqZW"
      },
      "outputs": [],
      "source": [
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'scaler': scaler,\n",
        "    'feature_names': X_train.columns.tolist(),\n",
        "    'X_train_scaled': X_train_scaled,\n",
        "    'X_test_scaled': X_test_scaled,\n",
        "    'y_test': y_test\n",
        "}, 'kan_model_2.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bp3NA-P8GO5s"
      },
      "outputs": [],
      "source": [
        "def analyze_shap(model_path):\n",
        "    import torch\n",
        "    import shap\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    # Define KANWrapper\n",
        "    class KANWrapper:\n",
        "        def __init__(self, model, device):\n",
        "            self.model = model\n",
        "            self.device = device\n",
        "\n",
        "        def __call__(self, data):\n",
        "            self.model.eval()\n",
        "            with torch.no_grad():\n",
        "                data = torch.FloatTensor(data).to(self.device)\n",
        "                outputs = self.model(data)\n",
        "                probabilities = torch.softmax(outputs, dim=1)[:, 1].cpu().numpy()\n",
        "            return probabilities\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Load the checkpoint\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "\n",
        "    # Initialize the model with optimized hyperparameters\n",
        "    input_dim = len(checkpoint['feature_names'])\n",
        "    hidden_dims = [94, 55]  # Optimize edilmiş değerler\n",
        "    inner_dim = 37  # Optimize edilmiş değer\n",
        "    dropout_rate = 0.4669  # Optimize edilmiş değer\n",
        "\n",
        "    # Initialize model with optimized architecture\n",
        "    model = KolmogorovArnoldNetworkWithDropout(\n",
        "        input_dim=input_dim,\n",
        "        hidden_dims=hidden_dims,\n",
        "        inner_dim=inner_dim,\n",
        "        dropout_rate=dropout_rate\n",
        "    ).to(device)\n",
        "\n",
        "    # Load state dict\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.eval()\n",
        "\n",
        "    # Prepare background and explain data\n",
        "    background_data = checkpoint['X_train_scaled'][:100]  # Using scaled data\n",
        "    explain_data = checkpoint['X_test_scaled'][:100]  # Using scaled data\n",
        "\n",
        "    # Convert feature_names to a NumPy array\n",
        "    feature_names = np.array(checkpoint['feature_names'])\n",
        "\n",
        "    # Initialize SHAP KernelExplainer\n",
        "    wrapper = KANWrapper(model, device)\n",
        "    explainer = shap.KernelExplainer(wrapper, background_data)\n",
        "\n",
        "    # Calculate SHAP values\n",
        "    print(\"Calculating SHAP values (this may take a while)...\")\n",
        "    shap_values = explainer.shap_values(explain_data)\n",
        "\n",
        "    # Handle multi-class or binary classification\n",
        "    if isinstance(shap_values, list):\n",
        "        shap_values_plot = shap_values[1]\n",
        "    else:\n",
        "        shap_values_plot = shap_values\n",
        "\n",
        "    # Visualization: SHAP Summary Plot\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    shap.summary_plot(\n",
        "        shap_values_plot,\n",
        "        explain_data,\n",
        "        feature_names=feature_names,\n",
        "        show=False\n",
        "    )\n",
        "\n",
        "    # Feature Importance DataFrame\n",
        "    feature_importance = np.abs(shap_values_plot).mean(axis=0)\n",
        "    importance_df = pd.DataFrame({\n",
        "        'feature': feature_names,\n",
        "        'importance': feature_importance\n",
        "    }).sort_values('importance', ascending=False)\n",
        "\n",
        "    # Save the summary plot\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('shap_summary.png')\n",
        "    plt.close()\n",
        "\n",
        "    print(\"SHAP analysis completed. Results saved as 'shap_summary.png'\")\n",
        "    return shap_values, importance_df\n",
        "def plot_shap_importance(importance_df):\n",
        "    # Veriyi önemlilik sırasına göre sırala\n",
        "    df_sorted = importance_df.sort_values('importance', ascending=True)\n",
        "\n",
        "    # Görselleştirme\n",
        "    plt.figure(figsize=(12, 10))\n",
        "\n",
        "    # Barplot oluştur\n",
        "    bars = plt.barh(range(len(df_sorted)), df_sorted['importance'])\n",
        "\n",
        "    # Y ekseni etiketlerini ayarla\n",
        "    plt.yticks(range(len(df_sorted)), df_sorted['feature'])\n",
        "\n",
        "    # Başlık ve eksen etiketleri\n",
        "    plt.title('Feature Importance Based on SHAP Values', pad=20)\n",
        "    plt.xlabel('mean(|SHAP value|) (average impact on model output magnitude)')\n",
        "\n",
        "    # Çubukları mavi yap\n",
        "    for bar in bars:\n",
        "        bar.set_color('#1f77b4')  # Matplotlib'in varsayılan mavi rengi\n",
        "\n",
        "    # Izgarayı ekle ve stilini ayarla\n",
        "    plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
        "\n",
        "    # Layout'u düzenle\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Grafiği kaydet\n",
        "    plt.savefig('shap_importance_barplot.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    print(\"Barplot saved as 'shap_importance_barplot.png'\")\n",
        "\n",
        "# Görselleştirmeyi çalıştır\n",
        "plot_shap_importance(importance_df)\n",
        "# Run SHAP analysis\n",
        "shap_values, importance_df = analyze_shap('kan_model_2.pth')\n",
        "\n",
        "# Display feature importance\n",
        "print(\"\\nFeature Importance Rankings:\")\n",
        "print(importance_df)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}